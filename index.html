<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta content="Offcial website of 'AnyStory: Towards Unified Single and Multiple Subject Personalization in Text-to-Image Generation'"
          name="description">
    <meta content="AnyStory, Subject-Driven, Image Personalization" name="keywords">
    <meta content="width=device-width, initial-scale=1" name="viewport">

    <title>AnyStory</title>

    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-MRQC0YFE17');
    </script>

    <script src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js" type="module"></script>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
    <script src="https://code.jquery.com/jquery-1.11.0.min.js" type="text/javascript"></script>
    <script src="https://code.jquery.com/jquery-migrate-1.2.1.min.js" type="text/javascript"></script>
    <script src="https://unpkg.com/interactjs/dist/interact.min.js"></script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link href="./static/slick/slick.css" rel="stylesheet" type="text/css"/>
    <link href="./static/slick/slick-theme.css" rel="stylesheet" type="text/css"/>

    <link href="./static/css/bulma.min.css" rel="stylesheet">
    <link href="./static/css/bulma-slider.min.css" rel="stylesheet">
    <link href="./static/css/fontawesome.all.min.css" rel="stylesheet">
    <link href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" rel="stylesheet">
    <link href="./static/css/index.css" rel="stylesheet">

    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body>

<style>
    .container {
        max-width: 1280px;
        margin: 0 auto;
    }
</style>

<nav aria-label="main navigation" class="navbar" role="navigation">
    <div class="navbar-brand">
        <a aria-expanded="false" aria-label="menu" class="navbar-burger" role="button">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
        </a>
    </div>
    <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
            <a class="navbar-item" href="https://atchen.com">
        <span class="icon">
            <i class="fas fa-home"></i>
        </span>
            </a>

            <div class="navbar-item has-dropdown is-hoverable">
                <a class="navbar-link">
                    More Research
                </a>
                <div class="navbar-dropdown">
                    <a class="navbar-item" href="https://aigcdesigngroup.github.io/replace-anything/">
                        ReplaceAnything
                    </a>
                    <a class="navbar-item" href="https://aigcdesigngroup.github.io/transfer-anything/">
                        TransferAnything
                    </a>
                    <a class="navbar-item" href="https://aigcdesigngroup.github.io/homepage_anytext/">
                        AnyText
                    </a>
                    <a class="navbar-item" href="https://aigcdesigngroup.github.io/wordart.home/">
                        WordArt
                    </a>
                    <a class="navbar-item" href="https://aigcdesigngroup.github.io/UniPortrait-Page/">
                        UniPortrait
                    </a>
                </div>

            </div>
        </div>
    </div>
</nav>

<section class="hero">
    <div class="hero-body">
        <div class="container">

            <div class="container has-text-centered">

                <h1 class="title is-1 publication-title">
                    AnyStory: Towards Unified Single and Multiple Subject Personalization in Text-to-Image Generation
                </h1>

                <div class="is-size-5 publication-authors">
                    <div class="author-block">
                        <a href="https://github.com/junjiehe96">Junjie He</a>,
                        <a href="https://openreview.net/profile?id=~Yuxiang_Tuo2" target="_blank">Yuxiang Tuo</a>,
                        <a href="" target="_blank">Binghui Chen</a>,
                        <a href="" target="_blank">Chongyang Zhong</a>,
                        <a href="" target="_blank">Yifeng Geng</a>,
                        <a href="https://scholar.google.com/citations?user=FJwtMf0AAAAJ&hl=zh-CN">Liefeng Bo</a>
                    </div>
                </div>

                <div class="is-size-5 publication-authors">
                    <span class="author-block">Institute for Intelligent Computing, Alibaba Tongyi Lab</span>
                </div>

                <div class="column has-text-centered">
                    <div class="publication-links">
                        <span class="link-block">
                            <a class="external-link button is-normal is-rounded is-dark"
                               href="https://arxiv.org/abs/2501.09503">
                            <span class="icon">
                                <i class="ai ai-arxiv"></i>
                            </span>
                             <span>Arxiv</span>
                            </a>
                        </span>
                        <!--                        <span class="link-block">-->
                        <!--                <a class="external-link button is-normal is-rounded is-dark"-->
                        <!--                   href="">-->
                        <!--                  <span class="icon">-->
                        <!--                    <i class="fab fa-github"></i>-->
                        <!--                  </span>-->
                        <!--                  <span>GitHub</span>-->
                        <!--                </a>-->
                        <!--              </span>-->
                        <span class="link-block">
                            <a class="external-link button is-normal is-rounded is-dark"
                               href="https://modelscope.cn/studios/iic/AnyStory/summary">
                            <span class="icon">
                                <img src="static/figures/logo/modelscope.png" alt="魔搭">
                            </span>
                            <span>ModelScope (魔搭)</span>
                            </a>
                        </span>
                        <span class="link-block">
                            <a class="external-link button is-normal is-rounded is-dark"
                               href="https://huggingface.co/spaces/modelscope/AnyStory">
                            <span class="icon">
                                <img src="static/figures/logo/huggingface.png" alt="huggingface">
                            </span>
                            <span>HuggingFace Demo</span>
                            </a>
                        </span>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>

                <div class="content has-text-justified">
                    <p>
                        Recently, large-scale generative models have demonstrated outstanding text-to-image generation
                        capabilities. However, generating high-fidelity personalized images with specific subjects still
                        presents challenges, especially in cases involving multiple subjects. In this paper, we propose
                        AnyStory, a unified approach for personalized subject generation. AnyStory not only achieves
                        high-fidelity personalization for single subjects, but also for multiple subjects, without
                        sacrificing subject fidelity. Specifically, AnyStory models the subject personalization problem
                        in an "encode-then-route" manner. In the encoding step, AnyStory utilizes a universal and
                        powerful image encoder, i.e., ReferenceNet, in conjunction with CLIP vision encoder to achieve
                        high-fidelity encoding of subject features. In the routing step, AnyStory utilizes a decoupled
                        instance-aware subject router to accurately perceive and predict the potential location of the
                        corresponding subject in the latent space, and guide the injection of subject conditions.
                        Detailed experimental results demonstrate the excellent performance of our method in retaining
                        subject details, aligning text descriptions, and personalizing for multiple subjects.
                        <br>
                        <br>
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="method">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Method</h2>

                <div class="content has-text-justified">
                    <p>
                        AnyStory follows the "encode-then-route" conditional generation paradigm. It first utilizes a
                        simplified ReferenceNet combined with a CLIP vision encoder to encode the subject, and then
                        employs a decoupled instance-aware subject router to guide the subject condition injection. The
                        training process is divided into two stages: the subject encoder training stage and the router
                        training stage. For brevity, we omit the text conditional branch here.
                    </p>
                </div>

                <div class="container is-max-desktop">
                    <img alt="Image0" height="100%" src="./static/figures/approach/framework.jpg">
                    <br>
                    <br>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container">
        <h3 class="title has-text-centered">Example Generations</h3>
        <div class="carousel results-carousel carousel-control-prev carousel-inner" data-ride="carousel"
             id="results-carousel-a">
            <div>
                <div class="results-item is-max-desktop">
                    <img alt="Image0" height="100%" src="./static/figures/intro/examples-1.jpg">
                </div>
            </div>

            <div>
                <div class="results-item is-max-desktop">
                    <img alt="Image1" height="100%" src="./static/figures/intro/examples-2.jpg">
                </div>
            </div>

            <div>
                <div class="results-item is-max-desktop">
                    <img alt="Image2" height="100%" src="./static/figures/intro/examples-3.jpg">
                </div>
            </div>

        </div>

        <div class="content has-text-centered">
            <p align="center">
                Scroll for more examples.
            </p>
        </div>
    </div>
</section>

<!--
<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Experimental Results</h2>
                <br>
                <br>
                <div class="container is-max-desktop">
                    <img alt="Image0" src="./static/figures/expts/referencenet-encoder.jpg" width="80%">
                </div>
                <div class="content has-text-justified">
                    <p align="center">
                        Effect of ReferenceNet encoding. The ReferenceNet encoder enhances the preservation of subject
                        details.
                    </p>
                </div>

                <br>
                <br>
                <div class="container is-max-desktop">
                    <img alt="Image1" src="./static/figures/expts/router-effect.jpg" width="80%">
                </div>
                <div class="content has-text-justified">
                    <p align="center">
                        Effectiveness of the router. The router restricts the influence areas of the subject conditions,
                        thereby avoiding the blending of characteristics between multiple subjects and improving the
                        quality of the generated images.
                    </p>
                </div>

                <br>
                <br>
                <div class="container is-max-desktop">
                    <img alt="Image2" src="./static/figures/expts/router-visualize.jpg" width="80%">
                </div>
                <div class="content has-text-centered">
                    <p align="center">
                        Visualization of routing maps. We visualize the routing maps within each cross-attention layer
                        in the U-Net at different diffusion time steps. There are a total of 70 cross-attention layers
                        in the SDXL U-Net, and we sequentially display them in each subfigure in a top-to-bottom and
                        left-to-right order (yellow represents the effective region). We utilize 25 steps of EDM
                        sampling. Each complete row corresponds to one entity. The background routing map has been
                        ignored, which is the complement of the routing maps of all subjects. Best viewed in color and
                        zoomed in.
                    </p>
                </div>

                <br>
                <br>
                <div class="container is-max-desktop">
                    <img alt="Image3" src="./static/figures/expts/router-coarse-refine.jpg" width="80%">
                </div>

                <div class="content has-text-justified">
                    <p align="center">
                        Effectiveness of the proposed router structure. For the meaning of each illustration, please
                        refer to the above.
                    </p>
                </div>

            </div>
        </div>
    </div>
</section>
-->

<section class="section">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Acknowledgements</h2>

                <div class="content has-text-centered">
                    <p align="center">
                        All subject images referenced in this paper are sourced from Pixabay and Unsplash. We extend our
                        gratitude to the owners of these images for sharing their valuable assets.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section" id="BibTeX">
    <div class="container content is-max-desktop">
        <h2 class="title">BibTeX</h2>
        <pre><code>
            @article{he2025anystory,
                title={AnyStory: Towards Unified Single and Multiple Subject Personalization in Text-to-Image Generation},
                author={He, Junjie and Tuo, Yuxiang and Chen, Binghui and Zhong, Chongyang and Geng, Yifeng and Bo, Liefeng},
                journal={arXiv preprint arXiv:2501.09503},
                year={2025}
            }
      </code></pre>
    </div>
</section>

<footer class="footer">
    <div class="container">
        <div class="content has-text-centered">
            <p align="center">
                The website template is borrowed from HyperNeRF.
            </p>
        </div>
    </div>
</footer>

<script src="./static/slick/slick.js" type="text/javascript"></script>
</body>

</html>
